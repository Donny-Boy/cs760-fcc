{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.6.9\n",
      "Pandas 1.0.3\n",
      "NumPy 1.18.2\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "print('Python '+str(platform.python_version()))\n",
    "print('Pandas '+str(pd.__version__))\n",
    "print('NumPy '+str(np.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database without city_state\n",
    "\n",
    "Download the 'dataset.csv' to the folder 'db/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"dataset.csv\"\n",
    "\n",
    "df = pd.read_csv(\"db/\"+file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the data with 'not_commenter' =0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(index = df.loc[df['not_commenter']==0].index)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the target feature to be predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['not_commenter']\n",
    "df = df.drop(columns=['not_commenter'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the one-hot dataframe for campaigns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "campaigns = []\n",
    "for name in df.columns:\n",
    "    if \"campaign.\" in name:\n",
    "        campaigns.append(name)\n",
    "\n",
    "campaigns_df = df[campaigns]\n",
    "df = df.drop(columns=campaigns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe with the BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for w in df.columns:\n",
    "    if \"text.\" in w:\n",
    "        words.append(w)\n",
    "\n",
    "bow_df = df[words]\n",
    "df = df.drop(columns=words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe with the cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = []\n",
    "for c in df.columns:\n",
    "    if \"city_state.\" in c:\n",
    "        city.append(c)\n",
    "\n",
    "city_df = df[city]\n",
    "df = df.drop(columns=city)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert email hash to number of occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['email_hash']=df['email_hash'].replace(df['email_hash'].value_counts().to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_bow=(bow_df-bow_df.min())/(bow_df.max()-bow_df.min())\n",
    "normalized_df=(df-df.min())/(df.max()-df.min())\n",
    "\n",
    "bow_mm = (bow_df.max(),bow_df.min())\n",
    "df_mm = (df.max(),df.min())\n",
    "path = 'results/'\n",
    "pickle.dump(bow_mm, open( path+'bow_mm.p', \"wb\" ))\n",
    "pickle.dump(df_mm, open( path+'df_mm.p', \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_df = pd.concat([normalized_df[['date_disseminated','date_received','submitted','email_confirmation']]],axis=1)\n",
    "baseline_df.to_csv('db/B1.csv',index=False)\n",
    "\n",
    "baseline_cam_df = pd.concat([baseline_df,campaigns_df],axis=1)\n",
    "baseline_cam_df.to_csv('db/B2.csv',index=False)\n",
    "\n",
    "baseline_cam_bins_df = pd.concat([baseline_cam_df,normalized_df[['campaign_centered_bin','campaign_submitted_bin']]],axis=1)\n",
    "baseline_cam_bins_df.to_csv('db/B3.csv',index=False)\n",
    "\n",
    "baseline_bow_df = pd.concat([baseline_df,normalized_bow],axis=1)\n",
    "baseline_bow_df.to_csv('db/B4.csv',index=False)\n",
    "\n",
    "baseline_bow_bins_df = pd.concat([baseline_bow_df,normalized_df[['all_centered_bin','all_submitted_bin']]],axis=1)\n",
    "baseline_bow_bins_df.to_csv('db/B5.csv',index=False)\n",
    "\n",
    "y.to_csv('db/Y.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Database\n",
    "\n",
    "Download Rafaels PCA database. It was extracted to '../db/PCANC' here with the subdirectories:\n",
    "\n",
    "- '../db/PCANC/0/'\n",
    "- '../db/PCANC/1/'\n",
    "- '../db/PCANC/2/'\n",
    "- '../db/PCANC/3/'\n",
    "- '../db/PCANC/4/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'db/PCANC/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LR code uses the full database and the Cross Validation is already implemented and already. In the PCA database the data is separated in ten folds + test set, so we need to merge all to use later on the LR notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    Xfn = []\n",
    "    yfn = []\n",
    "    Xfn.append(path+str(i)+'/X_test_PCA_'+str(i)+'.csv')\n",
    "    yfn.append(path+str(i)+'/y_test_'+str(i)+'.csv')\n",
    "    for j in range(10):\n",
    "        Xfn.append(path+str(i)+'/X_PCA_'+str(i)+'_'+str(j+1)+'.csv')\n",
    "        yfn.append(path+str(i)+'/y_'+str(i)+'_'+str(j+1)+'.csv')\n",
    "    \n",
    "    for j in range(len(Xfn)):\n",
    "        X = Xfn\n",
    "        X = [pd.read_csv(f, header=None) for f in X]\n",
    "        X = pd.concat(X)\n",
    "        X.reset_index(drop=True, inplace=True)\n",
    "        y = yfn\n",
    "        y = [pd.read_csv(f, header=None) for f in y]\n",
    "        y = pd.concat(y)\n",
    "        y.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    X.to_csv('db/PCANC/XPCANC_'+str(i)+'.csv',index=False)\n",
    "    y.to_csv('db/PCANC/yPCANC_'+str(i)+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
